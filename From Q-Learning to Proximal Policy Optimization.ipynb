{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Q-Learning to Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Q-Learning to Policy Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the actions with different states, we want to find a function would approaximate the **excepted value** for each (action, state) pair, means $q(a, s)$ function. \n",
    "\n",
    "There are two kinds of methods to get the $q(a, s)$, one based on the whole episode and using Bellman function, and another uses one-step (Temporal Difference )to converge to $q^*(a, s)$. As we get the $q(a, s)$, we could use greedy or $\\epsilon$-greedy methods get each action when the agent faces different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some shortages when we get $q(a, s)$ firstly, and use its value get *best* or *$\\epsilon$-best* actions. \n",
    "\n",
    "1. WITHOUT REAL STOCHASTIC: There are so many occasions, such like an imperfectable information game, the best policy may be **stochastic**, we need to get the precisely probabiliy of each action, not just by greedy or $\\epsilon$-greedy. \n",
    "\n",
    "2. BRITTLE: $\\epsilon$-greedy selection may change dramatically for an arbitrarily small change in the estimated action values. \n",
    "\n",
    "3. HARD FOR CONTINUOUS: the $q(a, s)$ function is hard for continuous space and action problem. \n",
    "\n",
    "4. INFORMACTION IMPERFECT: because of our sensor limitation, there will be some situations that looks same, but actually are different. In such situations, we need different actions when face the *same* situations. \n",
    "\n",
    "Therefore, we need a methods which could get the probability of given **state-action** pair. For continous space, we could get the value of each index or categorical of actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the policy $\\pi(\\theta)$, for an episode, we want maxmize the **value** of **initial state** $s_0$, if we define the $J(\\theta) = v_{\\pi_{\\theta}(s_0)} $, our target is:\n",
    "$$argmax_{\\theta}{J(\\theta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the **tracjectories** to be a finite episode or continuous episode with horizon H. If we collect so many tracjectories $(\\tau^{0},\\tau^{1}, \\tau^{2},..., \\tau^{N})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for these tracjectories $J(\\theta) = \\sum_{\\tau}Pr(\\tau; \\theta)R(\\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\nabla {J(\\theta)} &= \\nabla {\\sum_{\\tau} Pr(\\tau; \\theta) R(\\tau)} \\\\\n",
    "&=   {\\sum_{\\tau} \\nabla {Pr(\\tau; \\theta) R(\\tau)}} \\\\\n",
    "&=   {\\sum_{\\tau} \\frac{Pr(\\tau; \\theta)} {Pr(\\tau; \\theta)} \\nabla {Pr(\\tau; \\theta) R(\\tau)}} \\\\\n",
    "&= \\sum_{\\tau} Pr(\\tau; \\theta) \\frac {\\nabla_{\\theta} Pr(\\tau; \\theta)} {Pr(\\tau; \\theta)} R(\\tau) \\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m}\\nabla_{\\theta} In(Pr(\\tau; \\theta))R(\\tau^i) \\text{  ; for sampled m tracjectories}  \\\\ \n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m} \\nabla_{\\theta} In(\\prod_{t=0}^{H} Pr(s_{t+1}^{i}| s_{t}^i, a_t^i) \\pi_{\\theta}(a_t^i | s_{t}^i)) R(\\tau^i)\\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m} \\nabla_{\\theta} \\left[{\\sum_{t = 0}^{H-1} In(Pr(s_{t+1}^i | s_t^i, a_t^i)) + \\sum_{i = 0}^{H} In(\\pi_{\\theta}(a_t^i | s_t^i))}]R(\\tau^i) \\right]\\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m} \\nabla_{\\theta} \\left[ \\sum_{t = 0}^{H-1} In(\\pi_{\\theta}(a_t^i | s_t^i))]R(\\tau^i) \\right]  \\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m}\\sum_{t = 0}^{H-1} \\nabla_{\\theta}In(\\pi_{\\theta}(a_t^i | s_t^i))R(\\tau^i) \\\\\n",
    "&\\propto \\frac{1}{m} \\sum_{i = 0 }^{m}\\sum_{t = 0}^{H-1} \\nabla_{\\theta}log(\\pi_{\\theta}(a_t^i | s_t^i))R(\\tau^i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the $\\nabla{J(\\theta)}$ as $\\hat{g}$, and for each step $t$, we approximate the reward $R(\\tau)$ as the **future reward** $R(\\tau^t)$, for a given sample, we get: \n",
    "\n",
    " $$ \\hat{g} = \\sum_{t = 0}^{H-1} \\nabla_{\\theta}log(\\pi_{\\theta}(a_t^i | s_t^i))\\sum_{t=t}^{H}r(s_{i, t}, a_{i, t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\theta} = \\theta +  \\alpha \\hat{g} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we could get sample ${s_i, a_i}$ for $\\pi(a | s)$: \n",
    "\n",
    "1. get the rewards $\\sum_{t=t}^{H}r(s_{i, t}, a_{i, t})$\n",
    "2. evalutate $\\nabla_{\\theta}{J(\\theta)} = \\nabla_{\\theta}log(\\pi_{\\theta}(a_t^i | s_t^i))\\sum_{t=t}^{H}r(s_{i, t}, a_{i, t})$\n",
    "3. $\\theta \\leftarrow \\theta + \\alpha \\nabla J_\\theta (\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  From Policy Gradients to Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the previous algorithm **REINFORCE**. The algorithm could start an arbitary $\\pi(\\theta) $, and based on the update, get better $\\theta$. \n",
    "\n",
    "But, the REINFORCE methods have high **variance**. \n",
    "\n",
    "why?\n",
    "\n",
    "because when we get the $\\theta$, we just could get limited actions from limited spaces. We could add some **baseline** to reduce the variance. \n",
    "\n",
    "If we use the value function **$v(s)$** to be the baseline, we could: \n",
    "\n",
    "1. get the action policy $\\pi_\\theta(\\theta)$, we called actor\n",
    "2. get the policy evaluation method $v_{w}(w)$, we called critic\n",
    "\n",
    "because of the $v(s)$ function, we could change our algorithm to an online algorithm, which doesn't need run an episode to get the rewards. \n",
    "\n",
    "\n",
    "**online actor-critic algorithm**\n",
    "\n",
    "1. take action$ a \\sim \\pi_{\\theta}(a | s) $, get $(s, a, s', r)$\n",
    "2. evaluate $\\hat{A}^{\\pi}(s, a) = r(s, a) + \\gamma v_{w}^{\\pi}(s') - v_{w}^{\\pi}(s)$\n",
    "3. $ w \\leftarrow  \\hat{A}^{\\pi}(s, a) \\nabla{V}(s, w) $\n",
    "4. $ \\nabla J(\\theta) = \\nabla log \\pi_{\\theta}(a | s) \\hat{A^{\\pi}}(s, a) $\n",
    "5. $ \\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add some methods to imporve the performance of this algorithm: \n",
    "\n",
    "1. N-step bootstrap\n",
    "2. Generalized Advantrage Estimation (GAE)\n",
    "3. A2C and A3C methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TRPO and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the REINFORCE or Actor-Critic methods, we get a sample and use it and drop out it. What if we can use the tracjectories that we collected over and over again, and import our policy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when we do policy gradient, we want to maximize the $J(\\theta) = \\mathbb{E}_{(\\tau; s_0, a_0, .. )} [\\sum_{t = 0} ^ {\\infty} \\gamma^t r(s_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are a new policy $\\pi(\\theta')$ , we take the actions are sampled from $\\pi(\\theta')$  to get tracjectory $\\tau $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\because A_{\\pi{\\theta}} &= {E}_{s' \\sim Pr(s' | s, a)}[r(s) + \\gamma V_{\\pi(\\theta)}(s') - V_{\\pi(\\theta)}(s)] \\\\\n",
    "\\therefore E_{\\tau | \\pi(\\theta')}[\\sum_{t = 0} ^ {\\infty} \\gamma^{t} A_{\\pi{\\theta}}] &= E_{\\tau | \\pi(\\theta')} [ \\sum_{t = 0}^{\\infty} \\gamma^{t}(r(s) + \\gamma V_{\\pi(\\theta)}(s') - V_{\\pi(\\theta)}(s))] \\\\\n",
    "&= E_{\\tau | \\pi(\\theta')} \\left[ \\sum_{t = 0}^{\\infty} (\\gamma^{t} r(s) + [\\gamma^t * \\gamma V_{\\pi(\\theta)}(s_{t+1)}) - \\gamma^t V_{\\pi}(s_t))]) \\right] \\\\\n",
    "&=E_{\\tau | \\pi(\\theta')} \\left[ \\sum_{t = 0}^{\\infty} (\\gamma^{t} r(s)) - V_{\\pi(\\theta)}(s_0) \\right] \\\\\n",
    "&=J(\\theta') - E_{\\tau | \\pi(\\theta')}V_{\\pi(\\theta)}(s_0)\\\\\n",
    "&=J(\\theta') - E_{s_o}[V_{\\pi(\\theta)}(s_0)] \\text{; because distribute $s_0$ is independent of $\\theta$}\\\\\n",
    "&=J(\\theta') - J(\\theta)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite it as: \n",
    "\n",
    "$$ J(\\theta') - J(\\theta) = E_{\\tau | \\pi(\\theta')}[\\sum_{t = 0} ^ {\\infty} \\gamma^{t} A_{\\pi{\\theta}}] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the $\\tau$ is consist on (action, state), based on the margin distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta') - J(\\theta) &= E_{\\tau | \\pi(\\theta')}[\\sum_t \\gamma^t A_{\\pi{\\theta}}]  \\\\\n",
    "&= \\sum_{t} E_{s_t \\sim p(\\theta)}  [E_{a_t \\sim \\pi(\\theta'(a_t | s_t))}[\\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t)]]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the sample importance: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "E_{x \\sim p(x) [f(x)]} &= \\int p(x) f(x) dx \\\\\n",
    "&= \\int \\frac{q(x)}{q(x)} p(x) f(x)dx \\\\\n",
    "&= \\int q(x)\\frac{p(x)}{q(x)}f(x) dx \\\\\n",
    "&= E_{x \\sim q(x)} \\left[\\frac{p(x)}{q(x)}f(x) \\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "J(\\theta') - J(\\theta) &= E_{\\tau | \\pi(\\theta')}[\\sum_t \\gamma^t A_{\\pi{\\theta}}]  \\\\\n",
    "&= \\sum_{t} E_{s_t \\sim p(\\theta')}  [E_{a_t \\sim \\pi(\\theta'(a_t | s_t))}[\\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t)]] \\\\\n",
    "&= \\sum_{t} E_{s_t \\sim p(\\theta')}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we know the $\\pi(\\theta)$, therefore, we could get the $E_{a_t \\sim \\pi_{ (\\theta) } (a_t | s_t)}$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current now, we did not know the $E_{s_t \\sim p(\\theta')}$, because the $\\theta'$ is the new policy paramters we want get. \n",
    "\n",
    "However, if we could approaximate the $p(\\theta')$ using $\\theta$, we could update the $J(\\theta)$ to $J(\\theta')$ over and over again, just using the $\\theta$, which we already know, and tracjectories we already have collected. \n",
    "\n",
    "It is to say, if we could make fellow be true: \n",
    "$$\\begin{align}\n",
    "J(\\theta') - J(\\theta) &= \\sum_{t} E_{s_t \\sim p(\\theta')}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right] \\\\\n",
    "&\\approx \\sum_{t} E_{s_t \\sim p(\\theta)}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right] \n",
    "\\end{align}$$\n",
    "if we define $\\bar{A}(\\theta') = \\sum_{t} E_{s_t \\sim p(\\theta)}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right]  $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the: \n",
    "\n",
    "$$\\theta' \\leftarrow argmax_{\\theta'} \\bar{A}(\\theta')$$\n",
    "\n",
    "to update $J(\\theta)$ as quick as possible: \n",
    "\n",
    "$$ J(\\theta') - J(\\theta) \\approx \\bar{A}(\\theta')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There brust out a question, when $E_{s_t \\sim p(\\theta')} \\approx E_{s_t \\sim p(\\theta')}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bounds of constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case1: if $\\pi_(\\theta)$ is a deterministic policy s.t $a_t = \\pi_\\theta(s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a policy $\\pi'$, which gives $a_t'$, define $P(a \\neq a') \\leq \\epsilon$ for all states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "P_{\\theta'}(s_t) &= (1 - \\epsilon)^t P_{\\theta} (s_t) + (1 - (1 - \\epsilon)^t) P_{diff}(s_t) \\\\\n",
    "|P_{\\theta'}(s_t) - p_{\\theta}(s_t)| &= (1 - (1 - \\epsilon)^t) |P_{diff}(s_t) - p_{\\theta}(s_t)| \\leq 2(1 - (1 - \\epsilon)^t) \\leq 2 \\epsilon t\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case2: $\\pi(\\theta)$ is a distribution\n",
    "\n",
    "for a policy $\\pi'$, which gives $a_t'$, define the two policy are **close** if $ |\\pi_{\\theta'}(a_t | s_t) - \\pi_{\\theta}(a_t | s_t)| \\leq \\epsilon$ for all states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align} \n",
    "E_{p_{\\theta'}(s_t)} [f(s_t)]= \\sum_{s_t} p_{\\theta'}(s_t)f(s_t) & \\geq \\sum_{s_t} \\left[  p_\\theta{s_t} - |p_{\\theta} - p_{\\theta'}(s_t)| max_{s_t}f(s_t)  \\right] \\\\\n",
    "&\\geq E_{p_{\\theta}(s_t)} \\left[ f(s_t) \\right] - 2\\epsilon t max_{s_t} f(s_t) \n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get $J(\\theta')$ with sampled importance: \n",
    "\n",
    "$$\\begin{align}\n",
    "&\\sum_{t} E_{s_t \\sim p(\\theta')}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right] \\geq \\\\\n",
    "&\\sum_{t} E_{s_t \\sim p(\\theta)}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right] - \\sum_{t} 2 \\epsilon t C\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the $C$ in above equation is the accumulated excepted rewards for all steps.\n",
    "\n",
    "$$ C \\sim O(\\frac{r_{max}}{ 1 - \\gamma}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current now, we get the bound that could use the previously collected tracjectories to update $\\theta$ to $\\theta'$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D_{TV}$ to $D_{KL}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the Pinsker's inequality, for p, q two probability distributions, then: \n",
    "\n",
    "$$ \\delta(p, q) \\leq \\sqrt{(\\frac{1}{2} D_{KL}(p | q))} \\text{  , with $D_{KL}(p | q) = \\sum_{i \\in X} \\left( log \\frac{p(i)} {q(i)} p(i)\\right)$} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could get the $|\\pi(\\theta) - \\pi(\\theta')| \\leq \\sqrt{\\frac{1}{2} D_{KL} (\\pi(\\theta) | \\pi(\\theta'))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous information, if we could define a small **$\\epsilon$**  and find the $\\theta'$ could satisfy following, we could update $\\theta$ to $\\theta'$ as fast as possible and keep the approximation. \n",
    "\n",
    "1. $\\theta'$ makes $\\sqrt{\\frac{1}{2} D_{KL} (\\pi(\\theta) | \\pi(\\theta') } \\leq \\epsilon$ to be true, which makes sure the approximate of two policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\theta'$ make $ \\sum_{t} E_{s_t \\sim p(\\theta)}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right] $ as large as possible, which could make $\\theta$ update to $\\theta'$ as fast as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the $D_{KL}(\\pi_{\\theta'}(a_t | s_t)|| (\\pi_{\\theta}(a_t | s_t)) \\leq \\epsilon$ **TRUST REGION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such as, We could using the following Loss function to satisfy the above two constrains: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}(\\theta', \\lambda) =  \\sum_{t} E_{s_t \\sim p(\\theta)}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right]  - \\lambda (D_{KL}(\\pi_{\\theta'}(a_t | s_t)|| (\\pi_{\\theta}(a_t | s_t))) - \\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updaing as the iteration: \n",
    "\n",
    "1. Maximize $\\mathcal{L}$ w.s.t to $\\theta$\n",
    "2. $\\lambda \\leftarrow \\lambda  + \\alpha \\lambda (D_{KL}(\\pi_{\\theta'}(a_t | s_t)|| (\\pi_{\\theta}(a_t | s_t))) - \\epsilon) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the $\\mathcal{L}(\\theta') = \\sum_{t} E_{s_t \\sim p(\\theta)}  \\left[ E_{a_t \\sim \\pi_{\\theta}}(a_t | s_t) \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)}{\\pi_{\\theta}(a_t, s_t)} \\gamma^{t} A_{\\pi{\\theta}} (s_t, a_t) \\right] \\right]  $ **surrgate loss function. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Taylor Approaximation, we could get: \n",
    "\n",
    "$$\\begin{align} \\mathcal{L}_{\\theta'} &\\approx \\mathcal{L}_{\\theta} + \\nabla_{\\theta'} \\mathcal{L}_{\\theta}(\\theta')(\\theta' - \\theta) \\\\\n",
    "D_{KL}(\\theta' || \\theta) &\\approx \\frac{1}{2} (\\theta' - \\theta)^T H(\\theta' - \\theta)\n",
    "\\approx \\frac{1}{2} \\| \\theta - \\theta'\\|^2 \\end{align}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the solution of this: \n",
    "\n",
    "$$ \\theta' = \\theta + \\sqrt{\\frac{\\epsilon}{\\| \\nabla_{\\theta}  \\mathcal{L(\\theta)} ^ 2\\|}}  \\nabla_{\\theta}  \\mathcal{L(\\theta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trust Region Policy Optimization Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with above update. \n",
    "1. Might not to robutst to turst region size $\\epsilon$; at some iteration $\\epsilon$ may be too large and performance and degrade\n",
    "2. Because of quadratic qpproximate, KL-divergence constraint may be violated \n",
    "\n",
    "Solution: \n",
    "1. Require imporovement in surrogate (make sure that $\\mathcal{L}_{\\theta}(\\theta') \\geq 0$)\n",
    "2. Enforce KL-constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute proposed policy $\\Delta_{k} = \\sqrt{\\frac{\\epsilon}{\\| \\nabla_{\\theta}  \\mathcal{L(\\theta)} ^ 2\\|}}  \\nabla_{\\theta}  \\mathcal{L(\\theta)}$\n",
    "2. for j = 0, 1, 2, ... , H, do:\n",
    "   > if $ L_(\\theta)(\\theta')\\geq 0 $ and $D_{KL}(\\theta | \\theta') \\leq \\delta$:\n",
    "   > > accept the update and set $ \\theta' = \\theta + \\alpha^j \\Delta_{k} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using $\\mathcal{L}(\\theta') = E \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)} {\\pi_{\\theta}(a_t, s_t)} A_{\\pi{\\theta}} (s_t, a_t) \\right]$ for one step $t$, \n",
    "\n",
    "the $\\frac{\\pi_{\\theta'}(a_t, s_t)} {\\pi_{\\theta}(a_t, s_t)} $ may to large and get the update too dramatically.  In order to solve this problem, the paper proposed two methods to solve this problem: \n",
    "\n",
    "### PPO-Clipped Surrogate Objective \n",
    "\n",
    "This methods change the $\\mathcal{L}$ to a clipped loss function.  let $r_{t}(\\theta) = \\frac{\\pi_{\\theta'}(a_t, s_t)} {\\pi_{\\theta}(a_t, s_t)}$:\n",
    "\n",
    "$$\\mathcal{L}^{CLIP}(\\theta) = \\mathbb{E}_t\\left[ min(r_{t}(\\theta), clip(r_t{\\theta}, 1 - \\epsilon, 1 + \\epsilon)\\bar{A_t})\\right]$$\n",
    "\n",
    "This method could let the updating not to small or to large. Could make the converagence more stable. \n",
    "\n",
    "### Adaptive KL Penalty Coefficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside clip the loss function, another way is using **Adaptive KL penalty Coefficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}^{KLPEN}(\\theta) = \\mathbb{E}_t  \\left[ \\frac{\\pi_{\\theta'}(a_t, s_t)} {\\pi_{\\theta}(a_t, s_t)} A_{t, \\pi{\\theta}}\\right] - \\beta D_{KL} \\left[ \\pi_{\\theta}{(\\cdot | s_t)}, \\pi_{\\theta'}{(\\cdot | s_t)}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $d = \\mathbb{E}_t D_{KL} \\left[ \\pi_{\\theta}{(\\cdot | s_t)}, \\pi_{\\theta'}{(\\cdot | s_t)} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if $ d < d_{targ} / 1.5, \\beta \\leftarrow \\beta / 2$\n",
    "- if $ d > d_{targ} * 1.5, \\beta \\leftarrow \\beta * 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dynamic $\\beta$ change, we could ensure the $D_{KL}$ to a suitable value. Which make the update not  too small or not too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conslusion\n",
    "\n",
    "Based on the TRPO and PPO methods, especially PPO, we could using the tracjectories more efficiently, and update the $\\theta$ to $\\theta'$ as fast as possible and make sure the stable when update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Trust region Policy Optimization: [https://paperswithcode.com/method/trpo]\n",
    "2. Proximal Policy Optimization: [https://paperswithcode.com/method/ppo]\n",
    "3. berkeley CS294: https://www.youtube.com/watch?v=ycCtmp4hcUs&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=14\n",
    "4. berkeley CS285: https://www.youtube.com/watch?v=QWnpF0FaKL4&list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc&index=41\n",
    "5. Reinforcement Learning An Intorduction, Sutton"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
