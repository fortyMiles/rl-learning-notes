{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2560a20",
   "metadata": {},
   "source": [
    "## Lesson-01: Finite Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820bee2",
   "metadata": {},
   "source": [
    "在现实生活中，我们常常会遇到该种类型的问题，既，它的解决方案需要一个过程，或者需要多步骤进行，而不是像我们的分类和预测任务一样，输入一个 x 给出一个 y 即可。这种问题复杂的地方在于，我们需要解决一系列 $x_0, x_1, x_2 .. , x_n $ \n",
    "\n",
    "这种问题的复杂点还有一个，我们期望解决某个问题，希望的是最终的结果，或者说这个整体的结果是最优的，那么，就会出现某种情况，例如，当在 $x_0$的时候采取了某个$y$, 这个$y$很高，但是此时的$y$很高，导致整个 **序列**很短，于是还不如在 $x_0$的时候并没有取得当时最大的$y$值，但是使得我们这个过程能够持续下去，最终取得了更大的回报。\n",
    "\n",
    "面对这种需要解决一系列决策问题，而且我们的结果最终是使得整体序列的最终回报最大的问题，叫做 reinforcement learning 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb24e5a",
   "metadata": {},
   "source": [
    "所以，RL问题我们需要解决的问题就是，找到一种方法，能够做出决定。那么，做出什么决定呢？ \n",
    "\n",
    "我们的环境Environment在每一个时间点$t$ 会让我们处于某种状态 $state_t$, 然后，当我们做出某个动作 $action_i$的时候，这个时候会给我们一个反馈的 $ reward_{t+1}$ 并且把我们的状态变换到 $state_{t+1}$ 对于很多场景，会到某种 **terminal** 的场景从而中止，有些场景就不会中止，可以一直持续下去。有明确的中止的场景叫做 **episodic process**  而对于持续的这种我们就叫做 **continous process** \n",
    "\n",
    "不论对于哪一个过程，我们抽象一下自己的目标其实就是希望从 $t = 0 $这个时候开始，我们希望能够让整体的 **rewards** 最大：\n",
    "\n",
    "$$ Goal = max(\\sum_{i \\in T}reward_t )$$\n",
    "\n",
    "那么，假设我们现在在某个时间点 $t$， 那么我们的“任务”就是让$t+1$之后的预期*rewards*总和能够最大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600899a",
   "metadata": {},
   "source": [
    "对于我们的Agent, 如果有有限的States, $ s \\in \\mathcal{S} $, 有限的动作选择 $ a \\in \\mathcal{A} $, rewards 也是有限的 $ r \\in \\mathcal{R} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8365ff",
   "metadata": {},
   "source": [
    "假设我们现在在某个状态 $s$，在这个状态下，我们知道采取不同的行动，能够给我们的reward是多少，我们将这个值标记为此时的 **quality**： \n",
    "\n",
    "$$ q(s, a) $$\n",
    "\n",
    "然后，对于不同的 actions, **q**值不一样，那么，我们如果对这个时刻，就要取 **q**最大的这个action，那么，我们将这个叫做“确定的policy”，即：\n",
    "\n",
    "$$ \\pi(s) = argmax_{a \\in \\mathcal{A}} q(s, a)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9777551",
   "metadata": {},
   "source": [
    "如果，我们在这个时刻，要取的action有很多种，那么，这个时候的policy就是一个函数: \n",
    "\n",
    "$$ \\pi(s, a) \\in [0, 1], for \\ a \\ \\in \\mathcal{A} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d11732",
   "metadata": {},
   "source": [
    "对于未来不同的行为、决策路径，对于同一个时态、同一个动作 $ q(s, a) $的估计和计算是不一样的，我们将 $ q(s, a) $最大的这个值成为*optimal q function*， 标记为 $q_{*}(s, a)$\n",
    "\n",
    "那么，我们这个时候，要想知道最优的policy，我们只需要选择一个动作，在这个动作下，*q*值最高，即：\n",
    "\n",
    "$$\\pi_{*}(s) = argmax_{a \\in \\mathcal{A}}q_{*}(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31656527",
   "metadata": {},
   "source": [
    "$\\star$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36a0d8",
   "metadata": {},
   "source": [
    "那么，知道了$\\pi_{*}(s)$能做什么呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f393ea2",
   "metadata": {},
   "source": [
    "对于某个状态s，假设它的下一个状态我们记为$s'$, 那么这个状态的 value 在policy $\\pi$下，值就是：\n",
    "\n",
    "$$ v_{\\pi}(s) = r + v_{\\pi}(s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165dfae6",
   "metadata": {},
   "source": [
    "我们似乎在这里知道了我们每一个要采取的最优动作是什么，因为有了**$\\pi_{*}$**， 但是在同一个状态**s**，采取的同一个动作**a**, 现实世界复杂的点就在于，接下来的**s\\'** 和 **r**都有可能是随机的，即是说，此时此刻的状态一致，用的动作也一致，就算接下来的状态一样，但是收到的此刻的reward也可能不一样，或者接下来我们获得的reward一样，但是state也可能不一样，所以我们需要将这个变量考虑进来：\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_{r \\in \\mathcal{R}, s \\in \\mathcal{S}}p(s', r | s, \\pi({s}))(r + v_{\\pi}(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87dfff",
   "metadata": {},
   "source": [
    "而且，如果我们的policy是在不同的states，可以随机的进行选择action，那么，此时还有一个新的变量，就是a，那么这个时候的*value*就成了：\n",
    "\n",
    "$$ v_{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\sum_{r \\in \\mathcal{R}, s \\in \\mathcal{S}}p(s', r | s, a)(r + v_{\\pi}(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5259b8a",
   "metadata": {},
   "source": [
    "而且，由于我们知道 $q_{*}(s, a)$，那么 value of s:\n",
    "\n",
    "$$ v_{\\pi}(s) = max_{a \\in \\mathcal{A}} q_{\\pi_*}(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2d617",
   "metadata": {},
   "source": [
    "那么，假设我们不知道最优的policy $\\pi$ 函数，对于某个状态，value的最优解可以通过遍历actions来实现：\n",
    "\n",
    "$$ v_{*}(s) = \\mathop{max}\\limits_{a} \\sum_{s' r} p(s', r | s, a)[ r + \\gamma v_{*}(s')]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace545b",
   "metadata": {},
   "source": [
    "如果有求解$q_{*}(s, a)$, 也可以化为类似的Bellman方程：\n",
    "\n",
    "$$ q_{*}(s, a)  = \\sum_{s', r} p(s', r | s, a) [r + \\gamma \\mathop{max}\\limits_{a'} q_{*}(s', a')] $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
