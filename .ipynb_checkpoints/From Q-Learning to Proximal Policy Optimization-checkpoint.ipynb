{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Q-Learning to Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Q-Learning to Policy Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the actions with different states, we want to find a function would approaximate the **excepted value** for each (action, state) pair, means $q(a, s)$ function. \n",
    "\n",
    "There are two kinds of methods to get the $q(a, s)$, one based on the whole episode and using Bellman function, and another uses one-step (Temporal Difference )to converge to $q^*(a, s)$. As we get the $q(a, s)$, we could use greedy or $\\epsilon$-greedy methods get each action when the agent faces different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some shortages when we get $q(a, s)$ firstly, and use its value get *best* or *$\\epsilon$-best* actions. \n",
    "\n",
    "1. WITHOUT REAL STOCHASTIC: There are so many occasions, such like an imperfectable information game, the best policy may be **stochastic**, we need to get the precisely probabiliy of each action, not just by greedy or $\\epsilon$-greedy. \n",
    "\n",
    "2. BRITTLE: $\\epsilon$-greedy selection may change dramatically for an arbitrarily small change in the estimated action values. \n",
    "\n",
    "3. HARD FOR CONTINUOUS: the $q(a, s)$ function is hard for continuous space and action problem. \n",
    "\n",
    "4. INFORMACTION IMPERFECT: because of our sensor limitation, there will be some situations that looks same, but actually are different. In such situations, we need different actions when face the *same* situations. \n",
    "\n",
    "Therefore, we need a methods which could get the probability of given **state-action** pair. For continous space, we could get the value of each index or categorical of actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the policy $\\pi(\\theta)$, for an episode, we want maxmize the **value** of **initial state** $s_0$, if we define the $J(\\theta) = v_{\\pi_{\\theta}(s_0)} $, our target is:\n",
    "$$argmax_{\\theta}{J(\\theta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the **tracjectories** to be a finite episode or continuous episode with horizon H. If we collect so many tracjectories $(\\tau^{0},\\tau^{1}, \\tau^{2},..., \\tau^{N})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for these tracjectories $J(\\theta) = \\sum_{\\tau}Pr(\\tau; \\theta)R(\\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\nabla {J(\\theta)} &= \\nabla {\\sum_{\\tau} Pr(\\tau; \\theta) R(\\tau)} \\\\\n",
    "&=   {\\sum_{\\tau} \\nabla {Pr(\\tau; \\theta) R(\\tau)}} \\\\\n",
    "&=   {\\sum_{\\tau} \\frac{Pr(\\tau; \\theta)} {Pr(\\tau; \\theta)} \\nabla {Pr(\\tau; \\theta) R(\\tau)}} \\\\\n",
    "&= \\sum_{\\tau} Pr(\\tau; \\theta) \\frac {\\nabla_{\\theta} Pr(\\tau; \\theta)} {Pr(\\tau; \\theta)} R(\\tau) \\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m}\\nabla_{\\theta} In(Pr(\\tau; \\theta))R(\\tau^i) \\text{  ; for sampled m tracjectories}  \\\\ \n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m} \\nabla_{\\theta} In(\\prod_{t=0}^{H} Pr(s_{t+1}^{i}| s_{t}^i, a_t^i) \\pi_{\\theta}(a_t^i | s_{t}^i)) R(\\tau^i)\\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m} \\nabla_{\\theta} [{\\sum_{t = 0}^{H-1} In(Pr(s_{t+1}^i | s_t^i, a_t^i)) + \\sum_{i = 0}^{H} In(\\pi_{\\theta}(a_t^i | s_t^i))}]R(\\tau^i)\\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m} \\nabla_{\\theta} [\\sum_{t = 0}^{H-1} In(\\pi_{\\theta}(a_t^i | s_t^i))]R(\\tau^i)  \\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 0 }^{m}\\sum_{t = 0}^{H-1} \\nabla_{\\theta}In(\\pi_{\\theta}(a_t^i | s_t^i))R(\\tau^i) \\\\\n",
    "&\\propto \\frac{1}{m} \\sum_{i = 0 }^{m}\\sum_{t = 0}^{H-1} \\nabla_{\\theta}log(\\pi_{\\theta}(a_t^i | s_t^i))R(\\tau^i)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the $\\nabla{J(\\theta)}$ as $\\hat{g}$, and for each step $t$, we approximate the reward $R(\\tau)$ as the **future reward** $R(\\tau^t)$, for a given sample, we get: \n",
    "\n",
    " $$ \\hat{g} = \\sum_{t = 0}^{H-1} \\nabla_{\\theta}log(\\pi_{\\theta}(a_t^i | s_t^i))\\sum_{t=t}^{H}r(s_{i, t}, a_{i, t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{\\theta} = \\theta +  \\alpha \\hat{g} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we could get sample ${s_i, a_i}$ for $\\pi(a | s)$: \n",
    "\n",
    "1. get the rewards $\\sum_{t=t}^{H}r(s_{i, t}, a_{i, t})$\n",
    "2. evalutate $\\nabla_{\\theta}{J(\\theta)} = \\nabla_{\\theta}log(\\pi_{\\theta}(a_t^i | s_t^i))\\sum_{t=t}^{H}r(s_{i, t}, a_{i, t})$\n",
    "3. $\\theta \\leftarrow \\theta + \\alpha \\nabla J_\\theta (\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  From Policy Gradients to Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the previous algorithm **REINFORCE**. The algorithm could start an arbitary $\\pi(\\theta) $, and based on the update, get better $\\theta$. \n",
    "\n",
    "But, the REINFORCE methods have high **variance**. \n",
    "\n",
    "why?\n",
    "\n",
    "because when we get the $\\theta$, we just could get limited actions from limited spaces. We could add some **baseline** to reduce the variance. \n",
    "\n",
    "If we use the value function **$v(s)$** to be the baseline, we could: \n",
    "\n",
    "1. get the action policy $\\pi_\\theta(\\theta)$, we called actor\n",
    "2. get the policy evaluation method $v_{w}(w)$, we called critic\n",
    "\n",
    "because of the $v(s)$ function, we could change our algorithm to an online algorithm, which doesn't need run an episode to get the rewards. \n",
    "\n",
    "\n",
    "**online actor-critic algorithm**\n",
    "\n",
    "1. take action$ a \\sim \\pi_{\\theta}(a | s) $, get $(s, a, s', r)$\n",
    "2. evaluate $\\hat{A}^{\\pi}(s, a) = r(s, a) + \\gamma v_{w}^{\\pi}(s') - v_{w}^{\\pi}(s)$\n",
    "3. $ \\nabla J(\\theta) = \\nabla log \\pi_{\\theta}(a | s) \\hat{A^{\\pi}}(s, a) $\n",
    "4. $ \\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add some methods to imporve the performance of this algorithm: \n",
    "\n",
    "1. N-step bootstrap\n",
    "2. Generalized Advantrage Estimation (GAE)\n",
    "3. A2C and A3C methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TRPO and PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
